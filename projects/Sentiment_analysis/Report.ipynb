{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of this project is to predict the sentiments of sentences. <br>\n",
    "A sentence sentiment is whether a sentence sounds positive, (for eg. \"This movie was excellent!\") or negative, (for eg. Special effects were terrible...\"). <br>\n",
    "We work with IMDB movies reviews dataset. Movies are rated from 1 to 10. We consider only the reviews with rating 1 (the worst ones) and 10 (the best ones).  <br>\n",
    "The assumption is that movies with rating 1 have negative sentiment and movies with rating 10 have a positive sentiment.  <br>\n",
    "An additional goal is to test some heuristic approaches such that after we spot a negation or an adverb we take some certain actions described in the last section. <br>\n",
    "In sentences like ones above (\"This movie was excellent!\" and \"Special effects were terrible...\"), you can feel that the words \"excellent\" and \"terrible\" are the most important. We'd like to extract those words, on which meaning depends the most. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train dataset consists of 4732 reviews with rating 10 and 5100 reviews with rating 1. <br>\n",
    "A simple classifier that always votes for one class gives us a baseline accuracy of 51.87 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model cannot work on raw data. We could have somebody writing like TThiSSS or ..making..to..:much;.. punctuation marks. <br>\n",
    "Apart from obvious procedures like removing those punctuation marks and lowering all sentences, we try different preprocessing techniques such as: <br>\n",
    "\n",
    "1. Stemming, (reducing words to their root form, we decrease the size of words in our dictionary). \n",
    "2. Stop words removal, (we assume that words like 'the', 'and', 'a/an' etc. don't give much information).\n",
    "3. N-grams, (considering contiguous sequences of n items from a given sample of text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CountVectorizer \n",
    "2. Naive Bayes\n",
    "3. Logistic Regression\n",
    "4. SVM\n",
    "5. Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first question is how do we want to keep our data. Most of the models work which data points. <br>\n",
    "Points are usually living in some space. We can represent a point easily using a tuple/vector (x,y,z, ... ),  <br>\n",
    "where consecutive elements of this tuple correspond to value on consecutive axes. <br>\n",
    "For example, a point on a 2D plane can be described as a pair (x, y). <br>\n",
    "<br>\n",
    "So how to transform a sentence to a vector? <br>\n",
    "The simple idea is to use bag-of-words model. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.  <br>\n",
    "Basically, we have vectors of the size of the whole dictionary that we are using. <br>\n",
    "Every position in this vector corresponds to a different word from the dictionary. <br>\n",
    "When let's say a word occurs twice in our text, there should be 2 on the position that corresponds to this word. <br>\n",
    "<br>\n",
    "We want to make those vectors as short as possible to save memory. <br>\n",
    "If we had 10000 words in our dataset every vector would have to be that long. <br>\n",
    "Sentences usually have several dozen words so those vectors are mainly filled with zeros. <br>\n",
    "That's why we use methods like stemming which decrease the number of words significantly. Every conjunction of the word comes back to its root.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to test the simplest model, which works very well when it comes to classifying text documents.<br>\n",
    "In this model, there is no need to use bag-of-words, because we can keep occurrences of words in  Python dictionaries. <br>\n",
    "A Naive Bayes classifier uses the Bayes theorem to classify data.  <br>\n",
    "\n",
    "Bayes theorem:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}$$\n",
    "\n",
    "Let $c=1$ be class of good movies with review 10. <br>\n",
    "Given sentace \"this is a good movie\" we can say, that:\n",
    "\n",
    "$$ P(c=1 | \\text{this is a good movie}) = \\frac{P(\\text{this is a good movie}|c=1) * P(c=1)}{P(\\text{this is a good movie})}  $$\n",
    "where $ P(\\text{this is a good movie}) $ is a constant equal for both classes so we can skip it.\n",
    "\n",
    "Going further we can assume that\n",
    "\n",
    "$$ P(\\text{this is a good movie}|c=1) * P(c=1) \\approx P(\\text{this}|c=1) * P(\\text{is}|c=1) * P(\\text{a}|c=1) * P(\\text{good}|c=1) * P(\\text{movie}|c=1) * P(c=1) $$\n",
    "\n",
    "We can use our training data to compute $P(c | \\text{word}) $ for all classes. <br>\n",
    "Then we just choose the class, which has the highest probability. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we have to tune the alpha parameter. <br>\n",
    "Alpha parameter corresponds to the technique called Laplace smoothing. <br>\n",
    "Let's say we train our model on some data and when we predict some sentence there is a word that our model hasn't seen yet. <br>\n",
    "We cannot give it a 0 occurrence, because it would zero our probability. <br>\n",
    "The assumption is that each word in the vocabulary was seen at least or even a fraction of times in each kind of document.<br>\n",
    "This fraction is called alpha parameter.\n",
    "\n",
    "![alt text](Images/naive_bayes1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot:\n",
    "1. Accuracy - the percentage of correctly classified data.\n",
    "2. FN - False Negatives - the percentage of the data that was incorrectly classified as negative, (bad rating) when it should be classified as positive, (good rating).\n",
    "3. FP - False Positives - the percentage of the data that was incorrectly classified as positive, (good rating) when it should be classified as negative, (bad rating)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Naive Bayes accuracy. <br>\n",
    "![alt text](Images/naive_bayes2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklean Multinomial  Naive Bayes accuracy. <br>\n",
    "![alt text](Images/naive_bayes3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's. <br>\n",
    "![alt text](Images/naive_bayes4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes worked better than we expected. <br>\n",
    "With some parameters tuning it scored even better than sklearn implementation. <br>\n",
    "This simple model set the bar quite high yielding 89.95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of logistic regresion is to calculate probability of the sample $x$ belonging to class $y$.\n",
    "$$p(y=1|x) = \\sigma(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}$$  \n",
    "\n",
    "We can observe that:  \n",
    "$$ p(y=y^{(i)}|x^{(i)};\\Theta) = \\sigma(\\Theta^Tx)^{y^{(i)}}(1-\\sigma(\\Theta^Tx))^{(1-y^{(i)})}$$  \n",
    "\n",
    "Therefore the negative log likelihood ($nll$) is:$$\n",
    "\\begin{split}\n",
    "nll(\\Theta) &= -\\sum_{i=1}^{N} y^{(i)} \\log \\sigma(\\Theta^Tx) + (1-y^{(i)})\\log(1-\\sigma(\\Theta^Tx)) = \\\\\n",
    "&= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "So we are searching for $\\theta$:\n",
    "$$\\theta = arg\\,min_{\\theta} \\ nll(\\theta) $$  \n",
    "  \n",
    "We can further consider logistic regression with regularization, where:$$\n",
    "\\begin{split}\n",
    "nll(\\Theta) &= -\\sum_{i=1}^{N}y^{(i)}\\log p(y=1|x^{(i)}; \\Theta) + (1-y^{(i)})\\log  p(y=0|x^{(i)}; \\Theta) + \\frac{\\lambda}{2} \\sum_{i}\\theta_{i}^{2}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "There are a few ways to find $\\theta$. We will consider L-BFGS-B solver, then we will compare results with sklearn LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn Logistic Regression accuracy. <br>\n",
    "![alt text](Images/lr2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's. <br>\n",
    "![alt text](Images/lr3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing Logistic Regression we struggled mostly on complexity issues. <br>\n",
    "Sklearn implementation was running and converging much faster than ours. But this issue doesn't bother us, because minimizing functions is quite a complex task and they probably know more useful tricks.<br>\n",
    "Our model scored 89.92% and sklearn's 92.16%. <br>\n",
    "What is interesting our model worked better on stemmed data without stopwords contrary to sklearn. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngrams and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test if n-grams give us some more information and improve accuracy. <br>\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech.  <br>\n",
    "It means that we additionally consider continuous subsequences of length n instead of just single words. <br>\n",
    "\n",
    "We also check if using TF-IDF vectorizer yields better scores than CountVectorizer. <br>\n",
    "The TF-IDF vectorizer transforms a count matrix to a normalized tf or tf-idf representation. I.e. it is normalizing the count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between:\n",
    "1. Sklearn Logistic Regression + CV\n",
    "2. Sklearn Logistic Regression + CV + n-gram (1, 5)\n",
    "3. Sklearn Logistic Regression + TF-IDF + n-gram (1, 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "![alt text](Images/lr5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turned out using n-grams improves our accuracy a bit. <br>\n",
    "For ngram_range=(1,4) and every other, where the upper limit was greater than 4 we obtained 92.86% (+0.39%) accuracy for original data and 92.77% (+0.61%)  accuracy for stemmed data without stop words. <br>\n",
    "So n-grams are more effective if we do not stem and do not remove stop words. \n",
    "TF-IDF vectorizer wasn't better than CountVectorizer scoring around 3% less on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the SVM is to predict class of $x^{(i)}$ using $\\text{signum}(w^T x + b)$\n",
    "\n",
    "SVM try to find weights $w\\in\\mathbb{R}^n$ and bias $b\\in\\mathbb{R}$ that maximize the separation margin. This corresponds to solving the following quadratic optimization problem:\n",
    "\n",
    "$$\\begin{split}\n",
    "  \\min_{w,b,\\xi}  &\\frac{1}{2}w^Tw  + C\\sum_{i=1}^m \\xi_i  \\\\\n",
    "  \\text{s.t. } & y^{(i)}(w^T x^{(i)} + b) \\geq 1- \\xi_i\\;\\; \\forall_i \\\\\n",
    "  & \\xi_i \\geq 0 \\;\\; \\forall_i.\n",
    "\\end{split}$$\n",
    "\n",
    "We used dual form of this problem with kernel trick method, this corresponds to solving the following problem:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\min_{\\alpha}  \\frac{1}{2}  \\alpha^T \\mathbf{H}  \\alpha - 1^T \\alpha\n",
    "    \\\\\n",
    "     s.t.  - \\alpha_i \\leq 0 \n",
    "    \\\\\n",
    "      \\alpha_i \\leq C\n",
    "     \\\\\n",
    "     \\ y^T \\alpha = 0  \n",
    "     \\\\ \n",
    "     \\ H = y^TKy  \n",
    "     \\\\\n",
    "     \\ K_{i,j} = K(x_i,x_j)\n",
    "\\end{aligned}$$\n",
    "  \n",
    "We used qp solver to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our SVM accuracy.\n",
    "\n",
    "![alt text](Images/svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn SVM accuracy. <br>\n",
    "![alt text](Images/svm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our implementation and sklearn's on test.\n",
    "![alt text](Images/svm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM got the best accuracies so far. <br>\n",
    "With some parameters tuning our SVM was sometimes better than sklearns, which is something to be proud of. <br>\n",
    "It scored 92.16%, which is a bit better than Logistic Regression. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We came up with some heuristic approaches and implemented them in Naive Bayes. <br>\n",
    "\n",
    "1. If we come across a negation word, we negate ${k}$ words after it. I.e. we change the next ${k}$ words' probability for probability from another class.\n",
    "2. For words that enhance sentiment we multiply the next ${k}$ words' probability.\n",
    "3. There are also lists with positive and negative words, because sometimes we have a positive word from the negative class and vice versa. We swap probability in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracies we got. <br>\n",
    "\n",
    "![alt text](Images/heura1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between our Heuristic Naive Bayes and Naive Bayes. <br>\n",
    "![alt text](Images/heura2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Heuristic Naive Bayes can be considered as quite a success.<br> \n",
    "We can confirm our hypotheses about changing the weight of words after negation and\n",
    "enhancing the weight of some words after certain adverbs. <br>\n",
    "Also, it scored better than Naive Bayes, yielding solid 90.32% accuracy. <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this project we definitely learned a lot. <br>\n",
    "First of all, we are very happy with our results and final accuracies are a big success for us. <br>\n",
    "Our own implementations scored sometimes a bit better than sklearn's, <br>\n",
    "which we wouldn't believe in before the project started. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Images/res.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
