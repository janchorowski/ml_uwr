{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKY6us_cCg4"
   },
   "source": [
    "# Homework 2\n",
    "\n",
    "**For exercises in the week 18-19.11.20**\n",
    "\n",
    "**Points: 7 + 2bp bonus point**\n",
    "\n",
    "Please solve the problems at home and bring to class a [declaration form](http://ii.uni.wroc.pl/~jmi/Dydaktyka/misc/kupony-klasyczne.pdf) to indicate which problems you are willing to present on the backboard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q54xMuOWWsX7"
   },
   "source": [
    "## Problem 1 [1p]\n",
    "\n",
    "Consider a binary classification problem with discrete features, which we encode using the [one-hot](https://en.wikipedia.org/wiki/One-hot) encoding. E.g. suppose we have 2 features, $x_1\\in{a,b}$ and $x_2\\in{s,t,u}$. Then the encoded data is a real vector $x_e\\in\\mathbb{R}^5$:\n",
    "\n",
    "| $x_1$, $x_2$ | $x_e$ |\n",
    "|:------------:|:-----:|\n",
    "|     a,s      | 10100 |\n",
    "|     b,s      | 01100 |\n",
    "|     ...      |  ...  |\n",
    "|     b,u      | 01001 |\n",
    "\n",
    "Suppose you train a logistic regression classifier and a naive Bayes one.\n",
    "\n",
    "Show that the decision boundary of the two classifiers has the same form, i.e. that you could e.g. use a linear regression implementation to simulate the behavior of a naive Bayes model ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1ru6gzK-PR8"
   },
   "source": [
    "## Problem 2 (Weighted least squares) [1p + 1bp]\n",
    "\n",
    "Consider a least squares problem in which we apply a (known) weigth $w^{(i)}$ to each example:\n",
    "\n",
    "$$\n",
    "\\min_\\Theta \\frac{1}{2}\\sum_i w^{(i)}(x^{(i)}\\Theta - y^{(i)})^2.\n",
    "$$\n",
    "\n",
    "Write down the closed-form formula for predicting a single value at $x$.\n",
    "\n",
    "The weighted least squares are used in the Local \"[LOESS](https://en.wikipedia.org/wiki/Local_regression)\" regression: we find the set of closest neighbors to a query point, weight them based on the distance to the query, then predict the answer according to the weighted least squares. \n",
    "\n",
    "Describe the LOESS method for the bonus point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYFL1cWQbv1D"
   },
   "source": [
    "## Problem 3 (McKay 4.1) [1p]\n",
    "\n",
    "You are given a set of 12 balls in which:\n",
    "- 11 balls are equal\n",
    "- 1 ball is different (either heavier or lighter).\n",
    "\n",
    "You have a two-pan balance. How many weightings you must use to detect toe odd ball?\n",
    "\n",
    "*Hint:* A weighting can be seen as a random event. You can design them to maximize carry the most information, i.e. to maximize the entropy of their outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00IQ8eslcHVI"
   },
   "source": [
    "## Problem 4 (Murphy, 2.17) [1p]\n",
    "\n",
    "Expected value of the minimum.\n",
    "\n",
    "Let $X, Y$ be sampled uniformily on the interval $[0,1]$. What is the expected value of $\\min(X,Y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sYUtEqUQWYC"
   },
   "source": [
    "## Problem 5 [1p]\n",
    "\n",
    "Find the gradient with respect to $x$ of\n",
    "$-\\log(S(\\mathbf{x})_j)$, where $S$ is the\n",
    "softmax function (https://en.wikipedia.org/wiki/Softmax_function) and we are\n",
    "interested in the derivative over the $j$-th output of the Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0BwK5qnaxav"
   },
   "source": [
    "## Problem 6 [1p]\n",
    "\n",
    "Consider regresion problem, with $M$ predictors $h_m(x)$ trained to aproximate a target $y$. Define the error to be $\\epsilon_m(x) = h_m(x) - y$.\n",
    "\n",
    "Suppose you train $M$ independent classifiers with average least squares error\n",
    "$$\n",
    "E_{AV} = \\frac{1}{M}\\sum_{m=1}^M \\mathbb{E}_{x}[\\epsilon_m(x)^2].\n",
    "$$\n",
    "\n",
    "Further assume that the errors have zero mean and are uncorrelated:\n",
    "$$\n",
    "\\mathbb{E}_{x}[\\epsilon_m(x)] = 0\\qquad\\text{ and }\\qquad\\mathbb{E}_{x}[\\epsilon_m(x)\\epsilon_l(x)] = 0\\text{ for } m \\neq l\n",
    "$$\n",
    "\n",
    "Let the mean predictor be\n",
    "$$\n",
    "h_M(x) = \\frac{1}{M}\\sum_{m=1}^Mh_m(x).\n",
    "$$\n",
    "\n",
    "What is the average least squares error of $h_M(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ded8rFDBWXBz"
   },
   "source": [
    "## Problem 7: Numerical stability of SoftMax [1p]\n",
    "\n",
    "Many classifiers, such as Naive Bayes, multiply probabilities. To prevent loss of precision due to numerical underflow, we often prefer to add log-probabilities, rather than to multiply probabilities. However, we sometimes also need to add probabilities. This happens e.g. during normalization:\n",
    "- in Naive Bayes, to get final prbabilities we need to comput a sum-exp of the log-scores.\n",
    "- in Softmax, we again compute a sum-exp of the scores.\n",
    "\n",
    "Going back to logarithms reuires us to compute a log-sum-exp operation, or in other words using expoentiation, then addition, and finally re-application of the logarithm.\n",
    "\n",
    "Explain when the log-sum-exp operation may fail numerically, and how this failure can be prevented. Show how this fix can also be applied to the SoftMax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IzGTnZuMxTB"
   },
   "source": [
    "## Problem 8 [1bp]\n",
    "\n",
    "While on a walk, you notice that a locomotive has the serial number 50. Assuming that all locomotives used by PKP (the Polish railroad operator) are numbered using consecutive natural numbers, what is your estimate of $N$ the total number of locomotives operated by PKP?\n",
    "\n",
    "Tell why the Maximum Likelihood principle may not yield satisfactory results. \n",
    "\n",
    "Use the Bayesian approach to find the posterior distribution over\n",
    "  the number of locomotives. Then compute the expected count of\n",
    "  locomotives. For the prior use the power law:\n",
    "  \\begin{equation}\n",
    "  p(N) =  \\frac{1}{N^\\alpha}\\frac{1}{\\zeta(\\alpha,1)},\n",
    "  \\end{equation}\n",
    "  where the $\\zeta(s,q)=\\sum_{n=0}^{\\infty}\\frac{1}{(q+n)^s}$ is the\n",
    "  Hurwitz Zeta function\n",
    "  (https://en.wikipedia.org/wiki/Hurwitz_zeta_function)\n",
    "  available in Python as `scipy.special.zeta`. The use of the\n",
    "  power law is motivated by the observation that the frequency of\n",
    "  occurrence of a company is inversely proportional to its size (see\n",
    "  also: R.L. Axtell, Zipf distribution of US firm sizes\n",
    "  https://www.sciencemag.org/content/293/5536/1818).\n",
    "  \n",
    "  How would your estimate change after seeing 5 locomotives, with the\n",
    "  biggest serial number among them being 50?\n",
    "\n",
    "  **Note**: During the Second World War, a similar problem was\n",
    "  encountered while trying to estimate the total German tank\n",
    "  production from the serial numbers of captured machines. The\n",
    "  statistical estimates were the most precise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB9QqTlKFLH9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
